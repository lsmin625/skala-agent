{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph HITL (Human in the Loop)\n",
    "## ë‰´ìŠ¤ ê²€ìƒ‰ keyword ê°€ë“œë ˆì¼\n",
    "\n",
    "- **ë™ì  ì¸í„°ëŸ½íŠ¸ (Dynamic Interrupts)**: íŠ¹ì • ë…¸ë“œ ë‚´ì—ì„œ ì½”ë“œ ì‹¤í–‰ ì¤‘ ì¡°ê±´ì— ë”°ë¼ `interrupt()` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê·¸ë˜í”„ë¥¼ ì¼ì‹œ ì¤‘ì§€\n",
    "- **ì •ì  ì¸í„°ëŸ½íŠ¸ (Static Interrupts)**: ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•  ë•Œ `interrupt_before` ë˜ëŠ” `interrupt_after` ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë…¸ë“œì˜ ì‹¤í–‰ ì „í›„ì— í•­ìƒ ì¼ì‹œ ì¤‘ì§€ë˜ë„ë¡ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Annotated, Optional, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote.tools import GoogleNews\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "RISKY_KEYWORDS = [\n",
    "    \"ìì‚´\", \"ìí•´\", \"í­ë ¥\", \"ì‚´ì¸\", \"ê°•ê°„\",\n",
    "    \"19ê¸ˆ\", \"ì„±ì¸\", \"í¬ë¥´ë…¸\", \"ì•¼ë™\", \"ìŒë€\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë„êµ¬ ë° ìƒíƒœ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_news= GoogleNews()\n",
    "\n",
    "@tool\n",
    "def news_search(query: str) -> str:\n",
    "    \"\"\"ë‰´ìŠ¤ ê²€ìƒ‰ ë„êµ¬ë¥¼ í˜¸ì¶œí•´ ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.\"\"\"\n",
    "    results = google_news.search_by_keyword(query, k=5)\n",
    "    text = \"\\n\".join(\n",
    "        f\"- {r.get('content','(ë‚´ìš©ì—†ìŒ)')} | {r.get('url','')}\" for r in results\n",
    "    ) or \"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def contains_risky_term(text: str) -> bool:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì— ìœ„í—˜/ë¯¼ê° í‚¤ì›Œë“œê°€ í¬í•¨ëëŠ”ì§€ ê²€ì‚¬í•œë‹¤.\"\"\"\n",
    "    text = str(text or \"\").strip().lower()\n",
    "    return any(k in text for k in (kw.lower() for kw in RISKY_KEYWORDS))\n",
    "\n",
    "@tool\n",
    "def keyword_guardrail(query: str) -> str:\n",
    "    \"\"\"\n",
    "    ì§ˆì˜ì— ìœ„í—˜/ë¯¼ê° í‚¤ì›Œë“œê°€ í¬í•¨ëëŠ”ì§€ ê²€ì‚¬í•˜ê³ , í¬í•¨ ì‹œ ì‚¬ëŒì—ê²Œ ìŠ¹ì¸ì„ êµ¬í•˜ê¸° ìœ„í•´ ì‹¤í–‰ì„ ì¤‘ë‹¨í•œë‹¤.\n",
    "    - ìœ„í—˜ì–´ í¬í•¨: interrupt(...)ë¡œ ì¼ì‹œ ì¤‘ë‹¨ â†’ ì¬ê°œ í›„ 'APPROVED' ë˜ëŠ” 'DENIED' ë°˜í™˜\n",
    "    - ë¯¸í¬í•¨: 'SAFE' ë°˜í™˜\n",
    "    \"\"\"\n",
    "    if contains_risky_term(query):\n",
    "        resume_payload = interrupt({\"query\": query})\n",
    "        user_decision = str(resume_payload.get(\"data\", \"\")).strip().lower()\n",
    "        if user_decision in {\"ì˜ˆ\", \"yes\"}:\n",
    "            return \"APPROVED\"\n",
    "        return \"DENIED\"\n",
    "    return \"SAFE\"\n",
    "\n",
    "tools = [keyword_guardrail, news_search]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìƒíƒœ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ íƒ€ì… ì •ì˜: total=Falseë¡œ ì„ íƒì  í•„ë“œ ì§€ì •\n",
    "class AgentState(TypedDict, total=False):\n",
    "    messages: Annotated[list, \"LLM ë©”ì‹œì§€ ëª©ë¡\", add_messages]\n",
    "    result: Annotated[Optional[str], \"keyword_guardrail ì‘ë‹µ ë‚´ìš©\",]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê·¸ë˜í”„ ë…¸ë“œ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê²€ìƒ‰ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \"\n",
    "    \"ë¨¼ì € ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ í•­ìƒ 'keyword_guardrail' ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. \"\n",
    "    \"ê·¸ ê²°ê³¼ê°€ 'SAFE' ë˜ëŠ” 'APPROVED'ì´ë©´, ì›ë˜ ì‚¬ìš©ì ì§ˆë¬¸ì„ ê°€ì§€ê³  'news_search' ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. \"\n",
    "    \"ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°›ì€ í›„ì—ëŠ”, ê·¸ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. \"\n",
    "    \"ë§Œì•½ 'DENIED'ë¼ë©´, ì•ˆì „ ìƒì˜ ì´ìœ ë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•˜ì§€ ì•ŠëŠ”ë‹¤ê³  ì•Œë¦¬ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "def ensure_system_once(msgs: list[BaseMessage]) -> list[BaseMessage]:\n",
    "    # SystemMessageê°€ ê³„ì† ëˆ„ì ë˜ëŠ” ê²ƒì„ ë°©ì§€\n",
    "    if not msgs or not isinstance(msgs[0], SystemMessage):\n",
    "        return [SystemMessage(content=SYSTEM_PROMPT)] + msgs\n",
    "    return msgs\n",
    "\n",
    "# --- Nodes ---\n",
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"LLMì— ìƒíƒœì˜ ë©”ì‹œì§€ë“¤ì„ ì „ë‹¬í•˜ê³  ì‘ë‹µì„ ë°›ì•„ ìƒíƒœì— ì¶”ê°€\"\"\"\n",
    "    messages = ensure_system_once(state.get(\"messages\", []))\n",
    "    ai = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [ai]}\n",
    "\n",
    "def guardrail_extractor(state: AgentState) -> AgentState:\n",
    "    \"\"\"keyword_guardrail ë„êµ¬ ê²°ê³¼ë¥¼ ì¶”ì¶œí•˜ì—¬ state['result']ì— ì €ì¥\"\"\"\n",
    "    for msg in reversed(state.get(\"messages\", [])):\n",
    "        # ToolNode ì‹¤í–‰ í›„ ë„êµ¬ ì‘ë‹µì€ ToolMessage(name=<tool_name>) í˜•íƒœ\n",
    "        if isinstance(msg, ToolMessage) and msg.name == \"keyword_guardrail\":\n",
    "            return {\"result\": msg.content}  # 'SAFE' / 'APPROVED' / 'DENIED'\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.add_node(\"tools\", ToolNode(tools))\n",
    "graph.add_node(\"extractor\", guardrail_extractor)\n",
    "\n",
    "graph.add_edge(START, \"agent\")\n",
    "graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"tools\",   # tool_calls ìˆìœ¼ë©´ ToolNodeë¡œ\n",
    "        \"__end__\": END,     # ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "    },\n",
    ")\n",
    "graph.add_edge(\"tools\", \"extractor\")\n",
    "graph.add_edge(\"extractor\", \"agent\")\n",
    "\n",
    "\n",
    "# ì¸í„°ëŸ½íŠ¸ ì„¸ì´ë¸Œí¬ì¸íŠ¸ìš© ë©”ëª¨ë¦¬\n",
    "checkpointer = InMemorySaver()\n",
    "app = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UI êµ¬ì„±\n",
    "\n",
    "### ì…ì¶œë ¥ ì²˜ë¦¬ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_ai_text(state: dict) -> str:\n",
    "    \"\"\"ìƒíƒœì˜ ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ë‚´ìš©ì„ ë°˜í™˜í•œë‹¤.\"\"\"\n",
    "    msgs = state.get(\"messages\", [])\n",
    "    for m in reversed(msgs):\n",
    "        if isinstance(m, AIMessage):\n",
    "            return str(m.content or \"\")\n",
    "    return \"\"\n",
    "\n",
    "def submit_query(user_input, cfg_state, chat_history, pending_flag):\n",
    "    \"\"\"ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\"\"\"\n",
    "    if not user_input.strip():\n",
    "        return chat_history, cfg_state, gr.update(interactive=False), gr.update(interactive=False), pending_flag\n",
    "\n",
    "    cfg = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "    chat_history = chat_history + [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "    try:\n",
    "        result_state = app.invoke({\"messages\": [HumanMessage(content=user_input)]}, config=cfg)\n",
    "    except Exception as e:\n",
    "        chat_history += [{\"role\": \"assistant\", \"content\": f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}\"}]\n",
    "        return chat_history, cfg, gr.update(interactive=False), gr.update(interactive=False), False\n",
    "\n",
    "    interrupts = result_state.get(\"__interrupt__\", [])\n",
    "    if interrupts:\n",
    "        chat_history += [{\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"âš ï¸ ìœ„í—˜ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í• ê¹Œìš”? 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¥¼ ì„ íƒí•˜ì„¸ìš”.\\n\"\n",
    "        }]\n",
    "        return chat_history, cfg, gr.update(interactive=True), gr.update(interactive=True), True\n",
    "\n",
    "    # ì•ˆì „í•œ ê²½ìš° ê²°ê³¼ ì¶œë ¥\n",
    "    final_txt = get_last_ai_text(result_state) or \"ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    chat_history += [{\"role\": \"assistant\", \"content\": final_txt}]\n",
    "    return chat_history, cfg, gr.update(interactive=False), gr.update(interactive=False), False\n",
    "\n",
    "\n",
    "def resume_yes(cfg_state, chat_history, pending_flag):\n",
    "    \"\"\"â€˜ì˜ˆâ€™ ìŠ¹ì¸\"\"\"\n",
    "    if not cfg_state or not pending_flag:\n",
    "        return chat_history, gr.update(interactive=False), gr.update(interactive=False), False\n",
    "    try:\n",
    "        result_state = app.invoke(Command(resume={\"data\": \"ì˜ˆ\"}), config=cfg_state)\n",
    "        final_txt = get_last_ai_text(result_state) or \"ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        chat_history += [{\"role\": \"assistant\", \"content\": final_txt}]\n",
    "    except Exception as e:\n",
    "        chat_history += [{\"role\": \"assistant\", \"content\": f\"âš ï¸ ì¬ê°œ ì¤‘ ì˜¤ë¥˜: {e}\"}]\n",
    "    return chat_history, gr.update(interactive=False), gr.update(interactive=False), False\n",
    "\n",
    "\n",
    "def resume_no(cfg_state, chat_history, pending_flag):\n",
    "    \"\"\"â€˜ì•„ë‹ˆì˜¤â€™ ê±°ë¶€\"\"\"\n",
    "    if not cfg_state or not pending_flag:\n",
    "        return chat_history, gr.update(interactive=False), gr.update(interactive=False), False\n",
    "    try:\n",
    "        result_state = app.invoke(Command(resume={\"data\": \"ì•„ë‹ˆì˜¤\"}), config=cfg_state)\n",
    "        final_txt = get_last_ai_text(result_state) or \"ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
    "        chat_history += [{\"role\": \"assistant\", \"content\": final_txt}]\n",
    "    except Exception as e:\n",
    "        chat_history += [{\"role\": \"assistant\", \"content\": f\"âš ï¸ ì¬ê°œ ì¤‘ ì˜¤ë¥˜: {e}\"}]\n",
    "    return chat_history, gr.update(interactive=False), gr.update(interactive=False), False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UI êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"LangGraph HITL Guardrail\") as demo:\n",
    "    gr.Markdown(\"### ğŸ§© LangGraph HITL (ë‰´ìŠ¤ ê²€ìƒ‰ ê°€ë“œë ˆì¼) â€” ë™ì  ì¸í„°ëŸ½íŠ¸ ë°ëª¨\")\n",
    "\n",
    "    # âœ… type=\"messages\" ì ìš©\n",
    "    chatbot = gr.Chatbot(type=\"messages\", height=400)\n",
    "    user_input = gr.Textbox(label=\"ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”\", placeholder=\"ì˜ˆ: ìµœì‹  AI ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì¤˜\")\n",
    "    with gr.Row():\n",
    "        btn_submit = gr.Button(\"ì§ˆë¬¸ ë³´ë‚´ê¸°\", variant=\"primary\")\n",
    "        btn_yes = gr.Button(\"ì˜ˆ(ìŠ¹ì¸)\", interactive=False)\n",
    "        btn_no = gr.Button(\"ì•„ë‹ˆì˜¤(ê±°ë¶€)\", interactive=False)\n",
    "\n",
    "    st_cfg = gr.State(value=None)\n",
    "    st_pending = gr.State(value=False)\n",
    "    st_chat = gr.State(value=[])\n",
    "\n",
    "    # ì´ë²¤íŠ¸ ì—°ê²°\n",
    "    btn_submit.click(\n",
    "        submit_query,\n",
    "        inputs=[user_input, st_cfg, st_chat, st_pending],\n",
    "        outputs=[chatbot, st_cfg, btn_yes, btn_no, st_pending]\n",
    "    )\n",
    "\n",
    "    # btn_submit.click ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬\n",
    "    user_input.submit(\n",
    "        submit_query,\n",
    "        inputs=[user_input, st_cfg, st_chat, st_pending],\n",
    "        outputs=[chatbot, st_cfg, btn_yes, btn_no, st_pending]\n",
    "    )\n",
    "\n",
    "    btn_yes.click(\n",
    "        resume_yes,\n",
    "        inputs=[st_cfg, chatbot, st_pending],\n",
    "        outputs=[chatbot, btn_yes, btn_no, st_pending]\n",
    "    )\n",
    "\n",
    "    btn_no.click(\n",
    "        resume_no,\n",
    "        inputs=[st_cfg, chatbot, st_pending],\n",
    "        outputs=[chatbot, btn_yes, btn_no, st_pending]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "** End of Documents **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
