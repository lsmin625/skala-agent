{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph HITL (Human in the Loop)\n",
    "\n",
    "- **ë™ì  ì¸í„°ëŸ½íŠ¸ (Dynamic Interrupts)**: íŠ¹ì • ë…¸ë“œ ë‚´ì—ì„œ ì½”ë“œ ì‹¤í–‰ ì¤‘ ì¡°ê±´ì— ë”°ë¼ `interrupt()` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê·¸ë˜í”„ë¥¼ ì¼ì‹œ ì¤‘ì§€\n",
    "- **ì •ì  ì¸í„°ëŸ½íŠ¸ (Static Interrupts)**: ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•  ë•Œ `interrupt_before` ë˜ëŠ” `interrupt_after` ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë…¸ë“œì˜ ì‹¤í–‰ ì „í›„ì— í•­ìƒ ì¼ì‹œ ì¤‘ì§€ë˜ë„ë¡ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import gradio as gr\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "from langchain_teddynote.tools import GoogleNews\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages, BaseMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import interrupt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë„êµ¬ ë° ìƒíƒœ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# êµ¬ê¸€ ë‰´ìŠ¤ ë„êµ¬\n",
    "google_news= GoogleNews()\n",
    "\n",
    "# í‚¤ì›Œë“œ ë‰´ìŠ¤ ê²€ìƒ‰ ë„êµ¬ ìƒì„±\n",
    "@tool\n",
    "def search_keyword(query: str) -> list[dict[str, str]]:\n",
    "    \"\"\"Look up news by keyword\"\"\"\n",
    "    print(\"âœ… ë‰´ìŠ¤ ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰\")\n",
    "    return google_news.search_by_keyword(query, k=5)\n",
    "\n",
    "tools = [search_keyword]\n",
    "\n",
    "# ìƒíƒœ ì €ì¥ì†Œ ì •ì˜\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], \"ë©”ì‹œì§€ ëª©ë¡\", add_messages]\n",
    "    query: Annotated[str, \"ë‰´ìŠ¤ ê²€ìƒ‰ í‚¤ì›Œë“œ\"]\n",
    "\n",
    "# ì¸ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„°\n",
    "checkpointer = InMemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê·¸ë˜í”„ ë…¸ë“œ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_extractor(state: State):\n",
    "    \"\"\"ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ì–´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    last_message = state.get(\"messages\", [])[-1]\n",
    "    print(\"âœ… ë‰´ìŠ¤ ê²€ìƒ‰ì–´ ì¶”ì¶œ ì¤‘...\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ì— ê°€ì¥ ì í•©í•œ í‚¤ì›Œë“œë‚˜ ì§ˆë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ë‹¤ë¥¸ ë§ì€ í•˜ì§€ ë§ê³  í‚¤ì›Œë“œë§Œ ì •í™•íˆ ë°˜í™˜í•´ì¤ë‹ˆë‹¤.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    prompt_message = prompt.format_messages(question=last_message.content)\n",
    "    response = llm.invoke(prompt_message)\n",
    "    extracted_query = response.content.strip()\n",
    "    print(f\"âœ… ì¶”ì¶œëœ ê²€ìƒ‰ì–´: {extracted_query}\")\n",
    "    return {\"messages\": [AIMessage(content=extracted_query)]}\n",
    "\n",
    "def news_summarizer(state: State):\n",
    "    \"\"\"ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°›ì•„ AIê°€ ìš”ì•½í•˜ê³ , ë™ì  ì¸í„°ëŸ½íŠ¸ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\"\"\"\n",
    "    tool_output = state.get(\"messages\", [])[-1].content\n",
    "    print(\"âœ… ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì¤‘...\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.\"),\n",
    "        (\"human\", \"{tool_output}\")\n",
    "    ])\n",
    "    prompt_message = prompt.format_messages(tool_output=tool_output)\n",
    "    summary = llm.invoke(prompt_message).content\n",
    "    print(f\"âœ… ë‰´ìŠ¤ ìš”ì•½:\\n{summary}\")\n",
    "\n",
    "    # ë™ì  ì¸í„°ëŸ½íŠ¸: íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ ì‚¬ìš©ìì˜ í™•ì¸ì„ ë°›ê¸° ìœ„í•´ ì‹¤í–‰ì„ ë©ˆì¶¤\n",
    "    sensitive_keywords = [\"ë…¼ë€\", \"ë¯¼ê°\", \"ê°ˆë“±\", \"ìš°ë ¤\", \"ë¹„íŒ\"]\n",
    "    if any(keyword in summary for keyword in sensitive_keywords):\n",
    "        print(\"ğŸ”´ ë™ì  ì¸í„°ëŸ½íŠ¸ ë°œìƒ: ë¯¼ê° í‚¤ì›Œë“œ ê°ì§€\")\n",
    "        interrupt()\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=summary)]}\n",
    "\n",
    "def report_generator(state: State):\n",
    "    \"\"\"ìš”ì•½ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    summary = state.get(\"messages\", [])[-1].content\n",
    "    # ìµœì´ˆ ìš”ì²­ì„ ì°¾ê¸° ìœ„í•´ HumanMessage í•„í„°ë§\n",
    "    user_request = next((msg.content for msg in state.get(\"messages\", []) if isinstance(msg, HumanMessage)), \"\")\n",
    "    print(\"âœ… ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘...\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"ì‚¬ìš©ìì˜ ìµœì´ˆ ìš”ì²­ì‚¬í•­ì¸ '{user_request}'ì„ ì°¸ê³ í•˜ì—¬, ë‹¤ìŒ ë‰´ìŠ¤ ìš”ì•½ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜.\"),\n",
    "        (\"human\", \"{summary}\")\n",
    "    ])\n",
    "    prompt_message = prompt.format_messages(user_request=user_request, summary=summary)\n",
    "    report = llm.invoke(prompt_message).content\n",
    "    print(f\"âœ… ìƒì„±ëœ ë³´ê³ ì„œ:\\n{report}\")\n",
    "    return {\"messages\": [AIMessage(content=report)]}\n",
    "\n",
    "# tool ë…¸ë“œ ìƒì„±\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼ ---\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"query_extractor\", query_extractor)\n",
    "builder.add_node(\"news_search\", tool_node)\n",
    "builder.add_node(\"news_summarizer\", news_summarizer)\n",
    "builder.add_node(\"report_generator\", report_generator)\n",
    "\n",
    "builder.add_edge(START, \"query_extractor\")\n",
    "builder.add_edge(\"query_extractor\", \"news_search\")\n",
    "builder.add_edge(\"news_search\", \"news_summarizer\")\n",
    "builder.add_edge(\"news_summarizer\", \"report_generator\")\n",
    "builder.add_edge(\"report_generator\", END)\n",
    "\n",
    "# ì •ì /ë™ì  ì¸í„°ëŸ½íŠ¸ ì„¤ì •ê³¼ í•¨ê»˜ ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph = builder.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    interrupt_before=[\"news_search\"]\n",
    ")\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio UI ë° ì‹¤í–‰ ë¡œì§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_messages_to_ui(messages: list[HumanMessage | AIMessage]) -> list[dict[str, str]]:\n",
    "    \"\"\"LangChain ë©”ì‹œì§€ í˜•ì‹ì„ Gradio ì±—ë´‡ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    chat_history = []\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            chat_history.append({\"role\": \"user\", \"content\": msg.content})\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "    return chat_history\n",
    "\n",
    "def agent_process(user_input: str, thread_id: str):\n",
    "    \"\"\"ì‚¬ìš©ì ì…ë ¥ì— ë”°ë¼ LangGraph ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ìƒí˜¸ì‘ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    # HumanMessage ì¶”ê°€\n",
    "    message = HumanMessage(content=user_input)\n",
    "    \n",
    "    # ê·¸ë˜í”„ ì‹¤í–‰\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [message]},\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "    \n",
    "    for event in events:\n",
    "        snapshot = graph.get_state(config)\n",
    "        ui_messages = convert_messages_to_ui(snapshot.values[\"messages\"])\n",
    "        yield ui_messages\n",
    "\n",
    "def create_chatbot_response(message, history, thread_id_state):\n",
    "    \"\"\"Gradio ì±—ë´‡ì˜ ë©”ì¸ ì½œë°± í•¨ìˆ˜\"\"\"\n",
    "    thread_id = thread_id_state.value\n",
    "    \n",
    "    for chunk in agent_process(message, thread_id):\n",
    "        yield chunk\n",
    "\n",
    "def on_load():\n",
    "    \"\"\"UIê°€ ë¡œë“œë  ë•Œ ê³ ìœ í•œ thread_idë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    return gr.State(str(uuid.uuid4()))\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# LangGraph ë‰´ìŠ¤ ê²€ìƒ‰ ì—ì´ì „íŠ¸ (InMemorySaver ì‚¬ìš©)\")\n",
    "\n",
    "    thread_id_state = gr.State()\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        [],\n",
    "        elem_id=\"chatbot\",\n",
    "        height=300,\n",
    "        type=\"messages\",\n",
    "        placeholder=\"ë‰´ìŠ¤ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì˜ˆ: 'ì˜¤ëŠ˜ì˜ ì£¼ìš” ê²½ì œ ë‰´ìŠ¤'\"\n",
    "    )\n",
    "\n",
    "    chat_input = gr.Textbox(\n",
    "        show_label=False, \n",
    "        placeholder=\"ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆ„ë¥´ê±°ë‚˜ 'ë³´ë‚´ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.\", \n",
    "        container=False\n",
    "    )\n",
    "    \n",
    "    chat_input.submit(\n",
    "        create_chatbot_response,\n",
    "        [chat_input, chatbot, thread_id_state],\n",
    "        [chatbot],\n",
    "    )\n",
    "    \n",
    "    demo.load(on_load, inputs=[], outputs=[thread_id_state])\n",
    "\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "** End of Documents **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
