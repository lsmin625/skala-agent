#!/usr/bin/env python
# coding: utf-8
import uuid
import gradio as gr
from typing import Annotated, TypedDict, List, Optional

from langchain.schema import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages, BaseMessage
from langgraph.types import interrupt  # âœ… ì‹¤ì œ ì¼ì‹œì •ì§€

from langchain_teddynote.tools import GoogleNews
from dotenv import load_dotenv

load_dotenv()
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
google_news = GoogleNews()

# -----------------------------
# ìƒíƒœ ì •ì˜
# -----------------------------
class State(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    query: Optional[str]
    decision: Optional[str]   # "yes" | "no" | None

checkpointer = InMemorySaver()

# -----------------------------
# ë…¸ë“œë“¤
# -----------------------------
def query_extractor(state: State):
    """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    last = state["messages"][-1]
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ì— ê°€ì¥ ì í•©í•œ í‚¤ì›Œë“œë§Œ ë°˜í™˜í•˜ì„¸ìš”."),
        ("human", "{q}")
    ])
    extracted = llm.invoke(
        prompt.format_messages(q=last.content)
    ).content.strip()

    return {
        "query": extracted,
        "messages": [AIMessage(content=f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {extracted}")]
    }

def approval_gate(state: State):
    """
    ì§„ì§œ HITL: ì—¬ê¸°ì„œ ë©ˆì¶”ê³  ì‚¬ìš©ì ìŠ¹ì¸ ë°›ê¸°.
    - 1íšŒì°¨ ì‹¤í–‰: ìŠ¹ì¸ ì§ˆë¬¸ ë©”ì‹œì§€ë¥¼ ë‚¨ê¸°ê³  interrupt()
    - 2íšŒì°¨(ì¬ê°œ) ì‹¤í–‰: ìµœê·¼ ì‚¬ìš©ì ë©”ì‹œì§€ë¡œ 'ì˜ˆ/ì•„ë‹ˆì˜¤' íŒì • â†’ decision ì„¸íŒ…
    """
    # ìµœê·¼ ì‚¬ìš©ì ë‹µë³€ì— 'ì˜ˆ/ì•„ë‹ˆì˜¤'ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì‚¬
    recent_user = None
    for m in reversed(state["messages"]):
        if isinstance(m, HumanMessage):
            recent_user = m.content.strip().lower()
            break

    # ì•„ì§ ìŠ¹ì¸/ê±°ì ˆ ì˜ì‚¬ í‘œì‹œê°€ ì—†ìœ¼ë©´ ì§ˆë¬¸ ë„ìš°ê³  ì •ì§€
    if recent_user not in ("ì˜ˆ", "yes", "y", "ì•„ë‹ˆì˜¤", "no", "n"):
        ask = (
            "ğŸ”’ ë‰´ìŠ¤ ê²€ìƒ‰ì„ ì§„í–‰í•˜ê¸° ìœ„í•´ ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
            "ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? â€˜ì˜ˆâ€™ ë˜ëŠ” â€˜ì•„ë‹ˆì˜¤â€™ë¡œ ë‹µí•´ì£¼ì„¸ìš”."
        )
        # ë©”ì‹œì§€ë¥¼ ë‚¨ê¸°ê³  ì‹¤ì œë¡œ ì¤‘ì§€
        interrupt({"messages": [AIMessage(content=ask)]})
        # note: interrupt í›„ ë°˜í™˜ ì½”ë“œëŠ” ì‹¤í–‰ë˜ì§€ ì•Šì§€ë§Œ, íƒ€ì… ì¶©ì¡±ì„ ìœ„í•´ ê°’ ë¦¬í„´
        return {"messages": [AIMessage(content=ask)]}

    # ì‚¬ìš©ìê°€ ì‘ë‹µí–ˆë‹¤ë©´ decision ê¸°ë¡
    decision = "yes" if recent_user in ("ì˜ˆ", "yes", "y") else "no"
    return {"decision": decision}

def news_search(state: State):
    """ì‹¤ì œ ë‰´ìŠ¤ ê²€ìƒ‰"""
    q = state.get("query") or ""
    results = google_news.search_by_keyword(q, k=5)
    text = "\n".join([f"- {r.get('title','(ì œëª©ì—†ìŒ)')} | {r.get('link','')}" for r in results]) or "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
    return {"messages": [AIMessage(content=f"[ê²€ìƒ‰ê²°ê³¼]\n{text}")]}

def news_summarizer(state: State):
    """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½"""
    tool_output = state["messages"][-1].content
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”."),
        ("human", "{t}")
    ])
    summary = llm.invoke(
        prompt.format_messages(t=tool_output)
    ).content.strip()
    return {"messages": [AIMessage(content=summary)]}

def report_generator(state: State):
    """ìµœì¢… ë³´ê³ ì„œ ì‘ì„±"""
    messages = state["messages"]
    summary = next((m.content for m in reversed(messages) if isinstance(m, AIMessage)), "")
    user_request = next((m.content for m in messages if isinstance(m, HumanMessage)), "")
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ì‚¬ìš©ìì˜ ìµœì´ˆ ìš”ì²­ '{user_request}'ì„ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ìš”ì•½ìœ¼ë¡œ ê°„ê²°í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”."),
        ("human", "{summary}")
    ])
    report = llm.invoke(
        prompt.format_messages(user_request=user_request, summary=summary)
    ).content.strip()
    return {"messages": [AIMessage(content=report)]}

def aborted(state: State):
    """ì‚¬ìš©ìê°€ ê±°ì ˆí–ˆì„ ë•Œ ì¢…ë£Œ ë©”ì‹œì§€"""
    return {"messages": [AIMessage(content="ìš”ì²­ì— ë”°ë¼ ê²€ìƒ‰ì„ ì§„í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ ë³´ì„¸ìš”.")]}

# -----------------------------
# ì¡°ê±´ ë¼ìš°íŒ…
# -----------------------------
def route_after_approval(state: State) -> str:
    if state.get("decision") == "yes":
        return "news_search"
    else:
        return "aborted"

# -----------------------------
# ê·¸ë˜í”„ êµ¬ì„±
# -----------------------------
builder = StateGraph(State)

builder.add_node("query_extractor", query_extractor)
builder.add_node("approval_gate", approval_gate)
builder.add_node("news_search", news_search)
builder.add_node("news_summarizer", news_summarizer)
builder.add_node("report_generator", report_generator)
builder.add_node("aborted", aborted)

builder.add_edge(START, "query_extractor")
builder.add_edge("query_extractor", "approval_gate")
builder.add_conditional_edges("approval_gate", route_after_approval,
                              {"news_search": "news_search", "aborted": "aborted"})
builder.add_edge("news_search", "news_summarizer")
builder.add_edge("news_summarizer", "report_generator")
builder.add_edge("report_generator", END)
builder.add_edge("aborted", END)

graph = builder.compile(checkpointer=checkpointer)

# -----------------------------
# Gradio UI (ì¼ì‹œì •ì§€ â†’ ìŠ¹ì¸ â†’ ì¬ê°œ)
# -----------------------------
def to_ui(messages: List[HumanMessage | AIMessage]):
    chat = []
    for m in messages:
        if isinstance(m, HumanMessage):
            chat.append({"role": "user", "content": m.content})
        elif isinstance(m, AIMessage):
            chat.append({"role": "assistant", "content": m.content})
    return chat

def agent_process(user_input: str, thread_id: str):
    """
    - ë§¤ í˜¸ì¶œë§ˆë‹¤ ê°™ì€ thread_idë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ
      interrupt()ë¡œ ë©ˆì¶˜ ì§€ì ì—ì„œ ìë™ìœ¼ë¡œ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.
    """
    config = {"configurable": {"thread_id": thread_id}}

    # ìƒˆ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ë©° ì‹¤í–‰/ì¬ê°œ
    events = graph.stream({"messages": [HumanMessage(content=user_input)]},
                          config,
                          stream_mode="values")

    for _ in events:
        snap = graph.get_state(config)
        yield to_ui(snap.values["messages"])

def create_chatbot_response(message, history, thread_id_state):
    thread_id = thread_id_state.value
    for chunk in agent_process(message, thread_id):
        yield chunk

def on_load():
    return gr.State(str(uuid.uuid4()))

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# LangGraph ë‰´ìŠ¤ ê²€ìƒ‰ ì—ì´ì „íŠ¸ â€¢ HITL(ì¼ì‹œì •ì§€â†’ìŠ¹ì¸â†’ì¬ê°œ) ë°ëª¨")

    thread_id_state = gr.State()
    chatbot = gr.Chatbot([], elem_id="chatbot", height=360, type="messages",
                         placeholder="ì˜ˆ: 'ì˜¤ëŠ˜ì˜ ì£¼ìš” ê²½ì œ ë‰´ìŠ¤'")
    chat_input = gr.Textbox(show_label=False,
                            placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Enter",
                            container=False)

    chat_input.submit(create_chatbot_response, [chat_input, chatbot, thread_id_state], [chatbot])
    demo.load(on_load, inputs=[], outputs=[thread_id_state])

demo.launch(debug=True)
# demo.close()  # ë…¸íŠ¸ë¶/ì„œë²„ í™˜ê²½ì— ë§ê²Œ í•„ìš” ì‹œ ì‚¬ìš©
