#!/usr/bin/env python
# coding: utf-8

# # LangGraph HITL (Human in the Loop)
# 
# - **ë™ì  ì¸í„°ëŸ½íŠ¸ (Dynamic Interrupts)**: íŠ¹ì • ë…¸ë“œ ë‚´ì—ì„œ ì½”ë“œ ì‹¤í–‰ ì¤‘ ì¡°ê±´ì— ë”°ë¼ `interrupt()` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê·¸ë˜í”„ë¥¼ ì¼ì‹œ ì¤‘ì§€
# - **ì •ì  ì¸í„°ëŸ½íŠ¸ (Static Interrupts)**: ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•  ë•Œ `interrupt_before` ë˜ëŠ” `interrupt_after` ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë…¸ë“œì˜ ì‹¤í–‰ ì „í›„ì— í•­ìƒ ì¼ì‹œ ì¤‘ì§€ë˜ë„ë¡ ì„¤ì •

# In[ ]:


import uuid
import gradio as gr
from typing import Annotated, TypedDict

from langchain.schema import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain_teddynote.graphs import visualize_graph
from langchain_teddynote.tools import GoogleNews

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages, BaseMessage
from langgraph.prebuilt import ToolNode
from langgraph.types import interrupt

from dotenv import load_dotenv


load_dotenv()
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)


# ## ë„êµ¬ ë° ìƒíƒœ êµ¬ì„±

# In[ ]:


# êµ¬ê¸€ ë‰´ìŠ¤ ë„êµ¬
google_news= GoogleNews()

# í‚¤ì›Œë“œ ë‰´ìŠ¤ ê²€ìƒ‰ ë„êµ¬ ìƒì„±
@tool
def search_keyword(query: str) -> list[dict[str, str]]:
    """Look up news by keyword"""
    print("âœ… ë‰´ìŠ¤ ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰")
    return google_news.search_by_keyword(query, k=5)

tools = [search_keyword]

# ìƒíƒœ ì €ì¥ì†Œ ì •ì˜
class State(TypedDict):
    messages: Annotated[list[BaseMessage], "ë©”ì‹œì§€ ëª©ë¡", add_messages]
    query: Annotated[str, "ë‰´ìŠ¤ ê²€ìƒ‰ í‚¤ì›Œë“œ"]

# ì¸ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„°
checkpointer = InMemorySaver()


# ## ê·¸ë˜í”„ ë…¸ë“œ ì •ì˜

# In[ ]:


def query_extractor(state: State):
    """ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ì–´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    last_message = state.get("messages", [])[-1]
    print("âœ… ë‰´ìŠ¤ ê²€ìƒ‰ì–´ ì¶”ì¶œ ì¤‘...")

    prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ì— ê°€ì¥ ì í•©í•œ í‚¤ì›Œë“œë‚˜ ì§ˆë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ë‹¤ë¥¸ ë§ì€ í•˜ì§€ ë§ê³  í‚¤ì›Œë“œë§Œ ì •í™•íˆ ë°˜í™˜í•´ì¤ë‹ˆë‹¤."),
        ("human", "{question}")
    ])
    prompt_message = prompt.format_messages(question=last_message.content)
    response = llm.invoke(prompt_message)
    extracted_query = response.content.strip()
    print(f"âœ… ì¶”ì¶œëœ ê²€ìƒ‰ì–´: {extracted_query}")
    return {"messages": [AIMessage(content=extracted_query)]}

def news_summarizer(state: State):
    """ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°›ì•„ AIê°€ ìš”ì•½í•˜ê³ , ë™ì  ì¸í„°ëŸ½íŠ¸ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    tool_output = state.get("messages", [])[-1].content
    print("âœ… ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì¤‘...")
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤."),
        ("human", "{tool_output}")
    ])
    prompt_message = prompt.format_messages(tool_output=tool_output)
    summary = llm.invoke(prompt_message).content
    print(f"âœ… ë‰´ìŠ¤ ìš”ì•½:\n{summary}")

    # ë™ì  ì¸í„°ëŸ½íŠ¸: íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ ì‚¬ìš©ìì˜ í™•ì¸ì„ ë°›ê¸° ìœ„í•´ ì‹¤í–‰ì„ ë©ˆì¶¤
    sensitive_keywords = ["ë…¼ë€", "ë¯¼ê°", "ê°ˆë“±", "ìš°ë ¤", "ë¹„íŒ"]
    if any(keyword in summary for keyword in sensitive_keywords):
        print("ğŸ”´ ë™ì  ì¸í„°ëŸ½íŠ¸ ë°œìƒ: ë¯¼ê° í‚¤ì›Œë“œ ê°ì§€")
        interrupt()

    return {"messages": [AIMessage(content=summary)]}

def report_generator(state: State):
    """ìš”ì•½ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    summary = state.get("messages", [])[-1].content
    # ìµœì´ˆ ìš”ì²­ì„ ì°¾ê¸° ìœ„í•´ HumanMessage í•„í„°ë§
    user_request = next((msg.content for msg in state.get("messages", []) if isinstance(msg, HumanMessage)), "")
    print("âœ… ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ì‚¬ìš©ìì˜ ìµœì´ˆ ìš”ì²­ì‚¬í•­ì¸ '{user_request}'ì„ ì°¸ê³ í•˜ì—¬, ë‹¤ìŒ ë‰´ìŠ¤ ìš”ì•½ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜."),
        ("human", "{summary}")
    ])
    prompt_message = prompt.format_messages(user_request=user_request, summary=summary)
    report = llm.invoke(prompt_message).content
    print(f"âœ… ìƒì„±ëœ ë³´ê³ ì„œ:\n{report}")
    return {"messages": [AIMessage(content=report)]}

# tool ë…¸ë“œ ìƒì„±
tool_node = ToolNode(tools)


# ## ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼

# In[ ]:


# --- ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼ ---
builder = StateGraph(State)

builder.add_node("query_extractor", query_extractor)
builder.add_node("news_search", tool_node)
builder.add_node("news_summarizer", news_summarizer)
builder.add_node("report_generator", report_generator)

builder.add_edge(START, "query_extractor")
builder.add_edge("query_extractor", "news_search")
builder.add_edge("news_search", "news_summarizer")
builder.add_edge("news_summarizer", "report_generator")
builder.add_edge("report_generator", END)

# ì •ì /ë™ì  ì¸í„°ëŸ½íŠ¸ ì„¤ì •ê³¼ í•¨ê»˜ ê·¸ë˜í”„ ì»´íŒŒì¼
graph = builder.compile(
    checkpointer=checkpointer,
    interrupt_before=["news_search"]
)
visualize_graph(graph)


# ## Gradio UI ë° ì‹¤í–‰ ë¡œì§

# In[ ]:


def convert_messages_to_ui(messages: list[HumanMessage | AIMessage]) -> list[dict[str, str]]:
    """LangChain ë©”ì‹œì§€ í˜•ì‹ì„ Gradio ì±—ë´‡ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    chat_history = []
    for msg in messages:
        if isinstance(msg, HumanMessage):
            chat_history.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            chat_history.append({"role": "assistant", "content": msg.content})
    return chat_history

def agent_process(user_input: str, thread_id: str):
    """ì‚¬ìš©ì ì…ë ¥ì— ë”°ë¼ LangGraph ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ìƒí˜¸ì‘ìš©í•©ë‹ˆë‹¤."""
    config = {"configurable": {"thread_id": thread_id}}

    # HumanMessage ì¶”ê°€
    message = HumanMessage(content=user_input)

    # ê·¸ë˜í”„ ì‹¤í–‰
    events = graph.stream(
        {"messages": [message]},
        config,
        stream_mode="values",
    )

    for event in events:
        snapshot = graph.get_state(config)
        ui_messages = convert_messages_to_ui(snapshot.values["messages"])
        yield ui_messages

def create_chatbot_response(message, history, thread_id_state):
    """Gradio ì±—ë´‡ì˜ ë©”ì¸ ì½œë°± í•¨ìˆ˜"""
    thread_id = thread_id_state.value

    for chunk in agent_process(message, thread_id):
        yield chunk

def on_load():
    """UIê°€ ë¡œë“œë  ë•Œ ê³ ìœ í•œ thread_idë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return gr.State(str(uuid.uuid4()))


with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# LangGraph ë‰´ìŠ¤ ê²€ìƒ‰ ì—ì´ì „íŠ¸ (InMemorySaver ì‚¬ìš©)")

    thread_id_state = gr.State()

    chatbot = gr.Chatbot(
        [],
        elem_id="chatbot",
        height=300,
        type="messages",
        placeholder="ë‰´ìŠ¤ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì˜ˆ: 'ì˜¤ëŠ˜ì˜ ì£¼ìš” ê²½ì œ ë‰´ìŠ¤'"
    )

    chat_input = gr.Textbox(
        show_label=False, 
        placeholder="ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆ„ë¥´ê±°ë‚˜ 'ë³´ë‚´ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.", 
        container=False
    )

    chat_input.submit(
        create_chatbot_response,
        [chat_input, chatbot, thread_id_state],
        [chatbot],
    )

    demo.load(on_load, inputs=[], outputs=[thread_id_state])


demo.launch(debug=True)


# In[ ]:


demo.close()


# -----
# ** End of Documents **
